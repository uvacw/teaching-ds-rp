{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving and cleaning data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can be used to get the data from your experiment saved as a CSV\n",
    "file. You need to put the SQL file you receive in the phpmyadmin tool (see\n",
    "Canvas on instructions how to do this) and then run this code. Watch out!\n",
    "Sometimes you need to make small adaptations (indicated in the notebook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code connects to the SQL database from phpmyadmin and gets the\n",
    "names of all the tables that are present. For each group there should be several\n",
    "tables such as \"calibration\", \"userconsent\" or \"propositions\" -- but there might\n",
    "be some columns only present for some of the groups.\n",
    "\n",
    "**!! IMPORTANT !! You need to change the first command to reflect the name that\n",
    "you have given the database. In this example the database was called \"group25\",\n",
    "replace this with the name the database has in your phpmyadmin**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a connection to the MySQL database using SQLAlchemy\n",
    "engine = create_engine('mysql+pymysql://root:@localhost/group25')\n",
    "\n",
    "# Use the inspect module to get table names\n",
    "inspector = inspect(engine)\n",
    "tables = inspector.get_table_names()\n",
    "\n",
    "# Print the list of all tables\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next code cell we are selecting all the columns starting with \"user\"\n",
    "except for the \"users\" column (as it does not include any relevant information\n",
    "for us). Now we have a list of all the columns that we later want to retrieve\n",
    "data for. By taking this approach, our code works for all kinds of different\n",
    "columns starting with \"user\" since they are different from group to group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all the user columns\n",
    "usercols = [n for n in tables if n.startswith('user') and n!='users']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we also want to know to which statements the different likert scale items\n",
    "correspond (important for later analysis), we can retrieve a kind of \"codebook\"\n",
    "for this from the database. The information about the likert scale items can be\n",
    "stored in a few different tables: pre_propositions (asked before the\n",
    "calibration/recommendation), propositions (asked after the recommendation), and\n",
    "sometimes propositions1 if there were more questions than fit on the pages. Your\n",
    "group might have one, two, or all of these tables. Running the code below\n",
    "outputs the saved (pre-)propositions with their id, question wording, and scale\n",
    "(usually 5 or 7 point). You can use the id to look up the matching column in the\n",
    "csv dataframe later.\n",
    "\n",
    "With the code below we are retrieving the tables, loading them into pandas\n",
    "dataframes and adding a prefix to the item names (pre* or post*) as often the\n",
    "IDs are the same for prepropositions and propositions, so we want to be able to\n",
    "keep them apart later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'pre_propositions' in tables:    \n",
    "    # Query to select all items from userconsent table\n",
    "    query = \"SELECT * FROM pre_propositions\"\n",
    "\n",
    "    # Load data into a pandas DataFrame\n",
    "    pre_propositions = pd.read_sql(query, engine)\n",
    "    prefix = 'pre_'\n",
    "    string_columns = ['id']\n",
    "\n",
    "    for col in string_columns:\n",
    "        pre_propositions[col] = pre_propositions[col].apply(lambda x: f\"{prefix}{x}\")\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(pre_propositions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'propositions' in tables:        \n",
    "    # Query to select all items from userconsent table\n",
    "    query = \"SELECT * FROM propositions\"\n",
    "\n",
    "    # Load data into a pandas DataFrame\n",
    "    propositions = pd.read_sql(query, engine)\n",
    "    prefix = 'post_'\n",
    "    string_columns = ['id']\n",
    "\n",
    "    for col in string_columns:\n",
    "        propositions[col] = propositions[col].apply(lambda x: f\"{prefix}{x}\")\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(propositions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'propositions1' in tables:    \n",
    "    # Query to select all items from userconsent table\n",
    "    query = \"SELECT * FROM propositions1\"\n",
    "\n",
    "    # Load data into a pandas DataFrame\n",
    "    propositions1 = pd.read_sql(query, engine)\n",
    "    prefix = 'post_'\n",
    "    string_columns = ['id']\n",
    "\n",
    "    for col in string_columns:\n",
    "        propositions1[col] = propositions1[col].apply(lambda x: f\"{prefix}{x}\")\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(propositions1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we are defining a function that makes it possible for us later\n",
    "to also add a prefix to column names which we will use later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add prefix to all columns except 'userId'\n",
    "def add_prefix_except_userid(df, prefix):\n",
    "    return df.rename(columns={col: f\"{prefix}{col}\" if col != \"userId\" else col for col in df.columns})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we are now finally retrieving our data: We take the names\n",
    "of all the columns we identified as relevant (usercols), retrieve their content,\n",
    "and put them all in a list. There are two types of tables that first need some\n",
    "more work: userpresatisfaction(1) and usersatisfaction(1). These are the likert\n",
    "scale items and they are stored in so-called long format (meaning that every\n",
    "person appears in as many rows as are items in the likert scale, so if there is\n",
    "a likert scale with 14 items there are 14 rows per study participant). As the\n",
    "long format is not very useful for us to further work with the data, we convert\n",
    "it into wide format (you can see the command below, .pivot) so that all the\n",
    "tables are in the same format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for col in usercols:\n",
    "    query = f\"SELECT * FROM {col}\"\n",
    "    tablecontent = pd.read_sql(query, engine)\n",
    "    if col in ['userpresatisfaction', 'userpresatisfaction1']:\n",
    "        tablecontent = tablecontent.pivot(index='userId', columns='questionId', values='value').reset_index()\n",
    "        tablecontent.columns.name = None\n",
    "        add_prefix_except_userid(tablecontent, 'pre_')\n",
    "    elif col in ['usersatisfaction', 'usersatisfaction1']:\n",
    "        tablecontent = tablecontent.pivot(index='userId', columns='questionId', values='value').reset_index()\n",
    "        tablecontent.columns.name = None\n",
    "        add_prefix_except_userid(tablecontent, 'post_')\n",
    "    dfs.append(tablecontent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we merge all the tables we collected in the list together on userId so that\n",
    "we have one row per user with all the variables in one column each. We also\n",
    "further remove some variables that are not of interest to us (such as time\n",
    "variables) to further clean the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all DataFrames on the userId column\n",
    "merged_df = dfs[0]\n",
    "for df in dfs[1:]:\n",
    "    merged_df = pd.merge(merged_df, df, on='userId', how=\"outer\")\n",
    "merged_df = merged_df.loc[:,~merged_df.columns.str.startswith('time')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last step, we save the dataframe to a CSV file. We have now called this\n",
    "dataframe final_df.csv, you can change the name if you want -- just make sure\n",
    "you can find the dataframe after you have saved it. With this code, it is stored\n",
    "in the same folder as your Python script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('final_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

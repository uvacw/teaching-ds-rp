{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving and cleaning data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can be used to get the data from your experiment saved as a CSV\n",
    "file. You need to put the SQL file you receive in the phpmyadmin tool (see\n",
    "Canvas on instructions how to do this) and then run this code. Watch out!\n",
    "Sometimes you need to make small adaptations (indicated in the notebook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sqlalchemy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.0.30)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sqlalchemy) (4.10.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code connects to the SQL database from phpmyadmin and gets the\n",
    "names of all the tables that are present. For each group there should be several\n",
    "tables such as \"calibration\", \"userconsent\" or \"propositions\" -- but there might\n",
    "be some columns only present for some of the groups.\n",
    "\n",
    "**!! IMPORTANT !! You need to change the first command to reflect the name that\n",
    "you have given the database. In this example the database was called \"group25\",\n",
    "replace this with the name the database has in your phpmyadmin**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['calibration', 'pre_propositions', 'propositions', 'recommendation', 'userallergies', 'userconsent', 'userdemograph', 'usermancheck', 'userpresatisfaction', 'users', 'usersatisfaction', 'userselection']\n"
     ]
    }
   ],
   "source": [
    "# Create a connection to the MySQL database using SQLAlchemy\n",
    "engine = create_engine('mysql+pymysql://root:@localhost/group21')\n",
    "\n",
    "# Use the inspect module to get table names\n",
    "inspector = inspect(engine)\n",
    "tables = inspector.get_table_names()\n",
    "\n",
    "# Print the list of all tables\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next code cell we are selecting all the columns starting with \"user\"\n",
    "except for the \"users\" column (as it does not include any relevant information\n",
    "for us). Now we have a list of all the columns that we later want to retrieve\n",
    "data for. By taking this approach, our code works for all kinds of different\n",
    "columns starting with \"user\" since they are different from group to group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all the user columns\n",
    "usercols = [n for n in tables if n.startswith('user') and n!='users']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we also want to know to which statements the different likert scale items\n",
    "correspond (important for later analysis), we can retrieve a kind of \"codebook\"\n",
    "for this from the database. The information about the likert scale items can be\n",
    "stored in a few different tables: pre_propositions (asked before the\n",
    "calibration/recommendation), propositions (asked after the recommendation), and\n",
    "sometimes propositions1 if there were more questions than fit on the pages. Your\n",
    "group might have one, two, or all of these tables. Running the code below\n",
    "outputs the saved (pre-)propositions with their id, question wording, and scale\n",
    "(usually 5 or 7 point). You can use the id to look up the matching column in the\n",
    "csv dataframe later.\n",
    "\n",
    "With the code below we are retrieving the tables, loading them into pandas\n",
    "dataframes and adding a prefix to the item names (pre* or post*) as often the\n",
    "IDs are the same for prepropositions and propositions, so we want to be able to\n",
    "keep them apart later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                           question  scale\n",
      "0    pre_1                       I trust recommender systems.      5\n",
      "1    pre_2               I can depend on recommender systems.      5\n",
      "2    pre_3                  Recommender systems are reliable.      5\n",
      "3    pre_4  I trust recommender systems to recommend me al...      5\n",
      "4    pre_5  I trust the recommendations of the systems to ...      5\n",
      "5   pre_11  If a book or article is interesting, I do not ...      5\n",
      "6   pre_12    Without reading, my life would not be the same.      5\n",
      "7   pre_13  In comparison to other activities, reading is ...      5\n",
      "8   pre_14             Reading helps make my life meaningful.      5\n",
      "9   pre_15  I am confident I can understand difficult book...      5\n",
      "10  pre_16                                I am a good reader.      5\n",
      "11  pre_21         I would prefer complex to simple problems.      5\n",
      "12  pre_22  I like to have the responsibility of handling ...      5\n",
      "13  pre_23                    Thinking is not my idea of fun.      5\n",
      "14  pre_24  I would rather do something that requires litt...      5\n",
      "15  pre_25  I really enjoy a task that involves coming up ...      5\n",
      "16  pre_26  I would prefer a task that is intellectual, di...      5\n",
      "17  pre_31  In making decisions about things I might enjoy...      5\n",
      "18  pre_32  In making decisions about things I might enjoy...      5\n",
      "19  pre_33  In making decisions about things I might enjoy...      5\n"
     ]
    }
   ],
   "source": [
    "if 'pre_propositions' in tables:    \n",
    "    # Query to select all items from userconsent table\n",
    "    query = \"SELECT * FROM pre_propositions\"\n",
    "\n",
    "    # Load data into a pandas DataFrame\n",
    "    pre_propositions = pd.read_sql(query, engine)\n",
    "    prefix = 'pre_'\n",
    "    string_columns = ['id']\n",
    "\n",
    "    for col in string_columns:\n",
    "        pre_propositions[col] = pre_propositions[col].apply(lambda x: f\"{prefix}{x}\")\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(pre_propositions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id                                           question  scale\n",
      "0  post_1                  I am satisfied with the website.       5\n",
      "1  post_2  The website provided recommendations that matc...      5\n"
     ]
    }
   ],
   "source": [
    "if 'propositions' in tables:        \n",
    "    # Query to select all items from userconsent table\n",
    "    query = \"SELECT * FROM propositions\"\n",
    "\n",
    "    # Load data into a pandas DataFrame\n",
    "    propositions = pd.read_sql(query, engine)\n",
    "    prefix = 'post_'\n",
    "    string_columns = ['id']\n",
    "\n",
    "    for col in string_columns:\n",
    "        propositions[col] = propositions[col].apply(lambda x: f\"{prefix}{x}\")\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(propositions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'propositions1' in tables:    \n",
    "    # Query to select all items from userconsent table\n",
    "    query = \"SELECT * FROM propositions1\"\n",
    "\n",
    "    # Load data into a pandas DataFrame\n",
    "    propositions1 = pd.read_sql(query, engine)\n",
    "    prefix = 'post_'\n",
    "    string_columns = ['id']\n",
    "\n",
    "    for col in string_columns:\n",
    "        propositions1[col] = propositions1[col].apply(lambda x: f\"{prefix}{x}\")\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(propositions1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we are defining a function that makes it possible for us later\n",
    "to also add a prefix to column names which we will use later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add prefix to all columns except 'userId'\n",
    "def add_prefix_except_userid(df, prefix):\n",
    "    return df.rename(columns={col: f\"{prefix}{col}\" if col != \"userId\" else col for col in df.columns})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we are now finally retrieving our data: We take the names\n",
    "of all the columns we identified as relevant (usercols), retrieve their content,\n",
    "and put them all in a list. There are two types of tables that first need some\n",
    "more work: userpresatisfaction(1) and usersatisfaction(1). These are the likert\n",
    "scale items and they are stored in so-called long format (meaning that every\n",
    "person appears in as many rows as are items in the likert scale, so if there is\n",
    "a likert scale with 14 items there are 14 rows per study participant). As the\n",
    "long format is not very useful for us to further work with the data, we convert\n",
    "it into wide format (you can see the command below, .pivot) so that all the\n",
    "tables are in the same format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    userId  pre_1  pre_2  pre_3  pre_4  pre_5  pre_11  pre_12  pre_13  pre_14  \\\n",
      "0      485      4      2      2      2      2       2       2       2       2   \n",
      "1      488      4      2      2      2      2       2       2       2       2   \n",
      "2      489      3      2      2      2      2       2       2       2       2   \n",
      "3      492      3      2      2      2      2       2       2       2       2   \n",
      "4      497      3      3      2      2      2       2       2       2       2   \n",
      "5      499      3      2      2      2      2       2       2       2       2   \n",
      "6      504      3      2      2      2      2       2       2       2       2   \n",
      "7      505      3      2      2      2      2       2       2       2       2   \n",
      "8      506      3      3      2      2      2       2       2       2       2   \n",
      "9      507      3      2      2      2      2       2       2       2       2   \n",
      "10     508      3      2      3      2      2       3       2       2       2   \n",
      "11     510      4      2      2      2      2       2       2       2       2   \n",
      "12     511      4      2      2      2      2       2       2       2       2   \n",
      "13     512      3      3      2      2      2       2       2       2       2   \n",
      "14     513      2      2      2      2      2       2       2       2       2   \n",
      "15     514      2      2      2      2      2       2       2       2       2   \n",
      "16     516      3      3      3      3      3       3       2       2       2   \n",
      "17     517      2      2      2      2      2       2       2       2       2   \n",
      "18     518      3      2      2      2      2       2       2       2       2   \n",
      "19     519      3      3      3      3      3       2       2       2       2   \n",
      "20     520      3      2      2      2      2       2       2       2       2   \n",
      "21     521      4      2      2      2      2       2       2       2       2   \n",
      "22     522      4      3      3      2      2       2       2       2       2   \n",
      "23     523      2      2      2      2      2       3       2       2       2   \n",
      "24     524      3      2      2      2      2       2       2       2       2   \n",
      "25     527      3      2      2      2      2       2       2       2       2   \n",
      "26     528      2      2      2      2      2       2       2       2       2   \n",
      "27     529      2      2      2      2      2       2       2       2       2   \n",
      "28     530      3      2      2      2      2       2       2       2       2   \n",
      "29     531      1      2      2      2      2       2       2       2       2   \n",
      "30     532      3      3      2      2      2       2       2       2       2   \n",
      "\n",
      "    ...  pre_16  pre_21  pre_22  pre_23  pre_24  pre_25  pre_26  pre_31  \\\n",
      "0   ...       2       2       2       2       2       2       2       2   \n",
      "1   ...       2       2       2       2       2       2       2       2   \n",
      "2   ...       2       2       2       2       2       2       2       2   \n",
      "3   ...       2       2       2       2       2       2       2       2   \n",
      "4   ...       2       2       2       2       2       2       2       2   \n",
      "5   ...       2       2       2       2       2       2       2       2   \n",
      "6   ...       2       2       2       2       2       2       2       2   \n",
      "7   ...       2       2       2       2       2       2       2       2   \n",
      "8   ...       2       2       2       2       2       2       2       2   \n",
      "9   ...       2       2       2       2       2       2       2       2   \n",
      "10  ...       2       2       2       2       2       2       2       2   \n",
      "11  ...       2       2       2       2       2       2       2       2   \n",
      "12  ...       2       2       2       2       2       2       2       2   \n",
      "13  ...       2       2       2       2       2       2       2       2   \n",
      "14  ...       2       2       2       2       2       2       2       2   \n",
      "15  ...       2       2       2       2       2       2       2       2   \n",
      "16  ...       2       2       2       2       2       2       2       2   \n",
      "17  ...       2       2       2       2       2       2       2       2   \n",
      "18  ...       2       2       2       2       2       2       2       2   \n",
      "19  ...       2       2       2       2       2       2       2       2   \n",
      "20  ...       2       2       2       2       2       2       2       2   \n",
      "21  ...       2       2       2       2       2       2       2       2   \n",
      "22  ...       2       2       2       2       2       2       2       2   \n",
      "23  ...       2       2       2       2       2       2       2       2   \n",
      "24  ...       2       2       2       2       2       2       2       2   \n",
      "25  ...       2       2       2       2       2       2       2       2   \n",
      "26  ...       2       2       2       2       2       2       2       2   \n",
      "27  ...       2       2       2       2       2       2       2       2   \n",
      "28  ...       2       2       2       2       2       2       2       2   \n",
      "29  ...       2       2       2       2       2       2       2       2   \n",
      "30  ...       2       2       2       2       2       2       2       2   \n",
      "\n",
      "    pre_32  pre_33  \n",
      "0        2       2  \n",
      "1        2       2  \n",
      "2        2       2  \n",
      "3        2       2  \n",
      "4        2       2  \n",
      "5        2       2  \n",
      "6        2       2  \n",
      "7        2       2  \n",
      "8        2       2  \n",
      "9        2       2  \n",
      "10       2       2  \n",
      "11       2       2  \n",
      "12       2       2  \n",
      "13       2       2  \n",
      "14       2       2  \n",
      "15       2       2  \n",
      "16       2       2  \n",
      "17       2       2  \n",
      "18       2       2  \n",
      "19       2       2  \n",
      "20       2       2  \n",
      "21       2       2  \n",
      "22       2       2  \n",
      "23       2       2  \n",
      "24       2       2  \n",
      "25       2       2  \n",
      "26       2       2  \n",
      "27       2       2  \n",
      "28       2       2  \n",
      "29       2       2  \n",
      "30       2       2  \n",
      "\n",
      "[31 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "for col in usercols:\n",
    "    query = f\"SELECT * FROM {col}\"\n",
    "    tablecontent = pd.read_sql(query, engine)\n",
    "    if col in ['userpresatisfaction', 'userpresatisfaction1']:\n",
    "        tablecontent = tablecontent.pivot(index='userId', columns='questionId', values='value').reset_index()\n",
    "        tablecontent.columns.name = None\n",
    "        tablecontent = add_prefix_except_userid(tablecontent, 'pre_')\n",
    "        print(tablecontent)\n",
    "    elif col in ['usersatisfaction', 'usersatisfaction1']:\n",
    "        tablecontent = tablecontent.pivot(index='userId', columns='questionId', values='value').reset_index()\n",
    "        tablecontent.columns.name = None\n",
    "        tablecontent = add_prefix_except_userid(tablecontent, 'post_')\n",
    "    dfs.append(tablecontent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we merge all the tables we collected in the list together on userId so that\n",
    "we have one row per user with all the variables in one column each. We also\n",
    "further remove some variables that are not of interest to us (such as time\n",
    "variables) to further clean the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all DataFrames on the userId column\n",
    "merged_df = dfs[0]\n",
    "for df in dfs[1:]:\n",
    "    merged_df = pd.merge(merged_df, df, on='userId', how=\"outer\")\n",
    "merged_df = merged_df.loc[:,~merged_df.columns.str.startswith('time')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last step, we save the dataframe to a CSV file. We have now called this\n",
    "dataframe final_df.csv, you can change the name if you want -- just make sure\n",
    "you can find the dataframe after you have saved it. With this code, it is stored\n",
    "in the same folder as your Python script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('final_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
